# -*- coding: utf-8 -*-
"""
Trained Persian QA System using Hazm + TF-IDF
This system actually learns from the data using machine learning
"""

from hazm import Normalizer, sent_tokenize, word_tokenize, Stemmer, stopwords_list
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re


class TrainedPersianQA:
    """
    A QA system that learns from Persian text using:
    - Hazm for preprocessing
    - TF-IDF for feature extraction
    - Cosine similarity for matching
    """
    
    def __init__(self):
        """Initialize components"""
        print("ðŸš€ Initializing Trained Persian QA System...")
        
        # Hazm components
        self.normalizer = Normalizer()
        self.stemmer = Stemmer()
        
        # Get Persian stop words properly
        self.stop_words = set(stopwords_list())
        
        # Storage for training data
        self.sentences = []  # Original sentences
        self.processed_sentences = []  # Preprocessed sentences
        self.sentence_metadata = []  # Metadata (paragraph source, etc.)
        
        # TF-IDF vectorizer (will be trained)
        self.vectorizer = None
        self.sentence_vectors = None
        
        print("âœ“ System initialized")

    def preprocess_text(self, text, remove_stopwords=True):
        """
        Preprocess Persian text with Hazm
        
        Args:
            text: Raw Persian text
            remove_stopwords: Whether to remove stop words
            
        Returns:
            Preprocessed text string
        """
        # Normalize
        text = self.normalizer.normalize(text)
        
        # Tokenize
        words = word_tokenize(text)
        
        # Stem and filter
        processed_words = []
        for word in words:
            # Skip punctuation and numbers
            if re.match(r'^[\W\d]+$', word):
                continue
            
            # Stem the word
            stemmed = self.stemmer.stem(word)
            
            # Remove stop words if requested
            if remove_stopwords and stemmed in self.stop_words:
                continue
            
            processed_words.append(stemmed)
        
        return ' '.join(processed_words)
    
    def train(self, paragraphs, paragraph_names=None):
        """
        Train the QA system on a dataset of paragraphs
        
        Args:
            paragraphs: List of Persian text paragraphs
            paragraph_names: Optional names/IDs for paragraphs
        """
        print(f"\nðŸ“š Training on {len(paragraphs)} paragraphs...")
        
        if paragraph_names is None:
            paragraph_names = [f"Paragraph_{i+1}" for i in range(len(paragraphs))]
        
        # Process each paragraph
        for para_idx, (paragraph, para_name) in enumerate(zip(paragraphs, paragraph_names)):
            # Normalize paragraph
            normalized = self.normalizer.normalize(paragraph)
            
            # Split into sentences
            sentences = sent_tokenize(normalized)
            
            # Process each sentence
            for sent_idx, sentence in enumerate(sentences):
                # Skip very short sentences
                if len(word_tokenize(sentence)) < 3:
                    continue
                
                # Store original sentence
                self.sentences.append(sentence)
                
                # Preprocess and store
                processed = self.preprocess_text(sentence, remove_stopwords=True)
                self.processed_sentences.append(processed)
                
                # Store metadata
                self.sentence_metadata.append({
                    'paragraph_name': para_name,
                    'paragraph_idx': para_idx,
                    'sentence_idx': sent_idx,
                    'original': sentence
                })
        
        print(f"âœ“ Extracted {len(self.sentences)} sentences")
        
        # Train TF-IDF vectorizer
        print("ðŸ§  Training TF-IDF vectorizer...")
        self.vectorizer = TfidfVectorizer(
            max_features=1000,  # Keep top 1000 features
            ngram_range=(1, 2),  # Use unigrams and bigrams
            min_df=1,  # Minimum document frequency
        )
        
        # Fit and transform all sentences
        self.sentence_vectors = self.vectorizer.fit_transform(self.processed_sentences)
        
        print(f"âœ“ Trained on {len(self.vectorizer.get_feature_names_out())} features")
        print("âœ“ Training complete!")
        
        # Show learned vocabulary sample
        vocab = self.vectorizer.get_feature_names_out()
        print(f"   Sample vocabulary: {list(vocab[:10])}")
    
    def find_answer(self, question, top_k=4, threshold=0.1):
        """
        Find the best answer to a question
        
        Args:
            question: Persian question
            top_k: Number of candidate sentences to consider
            threshold: Minimum similarity threshold
            
        Returns:
            Dict with answer and metadata
        """
        if not self.vectorizer:
            return {
                'answer': "Ø³ÛŒØ³ØªÙ… Ù‡Ù†ÙˆØ² Ø¢Ù…ÙˆØ²Ø´ Ù†Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª!",
                'confidence': 0.0,
                'source': None
            }
        
        print(f"\nðŸ” Question: {question}")
        
        # Preprocess question
        processed_question = self.preprocess_text(question, remove_stopwords=True)
        print(f"   Processed: {processed_question}")
        
        # Convert question to vector
        question_vector = self.vectorizer.transform([processed_question])
        
        # Calculate similarity with all sentences
        similarities = cosine_similarity(question_vector, self.sentence_vectors)[0]
        
        # Get top-k most similar sentences
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        top_scores = similarities[top_indices]
        
        print(f"\n   Top {top_k} candidates:")
        for idx, score in zip(top_indices, top_scores):
            print(f"   [{score:.3f}] {self.sentences[idx][:60]}...")
        
        # Check if best match is above threshold
        best_idx = top_indices[0]
        best_score = top_scores[0]
        
        if best_score < threshold:
            return {
                'answer': "Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.",
                'confidence': best_score,
                'source': None
            }
        
        # Apply question-type specific logic
        answer_idx = self._select_best_answer(
            question, 
            top_indices, 
            top_scores,
            processed_question
        )
        
        return {
            'answer': self.sentences[answer_idx],
            'confidence': similarities[answer_idx],
            'source': self.sentence_metadata[answer_idx],
            'all_candidates': [
                {
                    'sentence': self.sentences[idx],
                    'score': float(similarities[idx])
                }
                for idx in top_indices
            ]
        }
    
    def _select_best_answer(self, question, candidate_indices, scores, processed_question):
        """
        Select the best answer using question-type heuristics
        
        Args:
            question: Original question
            candidate_indices: Indices of candidate sentences
            scores: Similarity scores
            processed_question: Preprocessed question
            
        Returns:
            Index of best answer
        """
        question_lower = question.lower()
        
        # Question type patterns
        why_patterns = ['Ú†Ø±Ø§', 'Ø¨Ù‡ Ú†Ù‡ Ø¯Ù„ÛŒÙ„', 'Ø¹Ù„Øª', 'Ø¯Ù„ÛŒÙ„']
        when_patterns = ['Ú©ÛŒ', 'Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ', 'Ú†Ù‡ Ø³Ø§Ù„ÛŒ', 'Ú©Ø¯Ø§Ù… Ø³Ø§Ù„']
        where_patterns = ['Ú©Ø¬Ø§', 'Ú©Ø¯Ø§Ù… Ù…Ú©Ø§Ù†', 'Ú©Ø¯Ø§Ù… Ø´Ù‡Ø±', 'Ø¯Ø± Ú©Ø¬Ø§']
        who_patterns = ['Ú†Ù‡ Ú©Ø³ÛŒ', 'Ú†Ù‡ Ú©Ø³Ø§Ù†ÛŒ', 'Ø¨Ù†ÛŒØ§Ù†Ú¯Ø°Ø§Ø±', 'Ù…ÙˆØ³Ø³']
        what_patterns = ['Ú†ÛŒØ³Øª', 'Ú†Ù‡ Ú†ÛŒØ²ÛŒ', 'Ú†Ú¯ÙˆÙ†Ù‡']
        
        question_type = None
        if any(p in question_lower for p in why_patterns):
            question_type = 'why'
        elif any(p in question_lower for p in when_patterns):
            question_type = 'when'
        elif any(p in question_lower for p in where_patterns):
            question_type = 'where'
        elif any(p in question_lower for p in who_patterns):
            question_type = 'who'
        elif any(p in question_lower for p in what_patterns):
            question_type = 'what'
        
        print(f"   Detected question type: {question_type}")
        
        # Boost scores based on question type
        boosted_scores = scores.copy()
        
        for i, idx in enumerate(candidate_indices):
            sentence = self.sentences[idx].lower()
            
            if question_type == 'why':
                # Look for reason indicators
                if any(word in sentence for word in ['Ø¯Ù„ÛŒÙ„', 'Ø²ÛŒØ±Ø§', 'Ú†ÙˆÙ†', 'Ø¨Ù‡ Ø¯Ù„ÛŒÙ„', 'Ø¨Ù‡ Ø®Ø§Ø·Ø±']):
                    boosted_scores[i] += 0.2
                    print(f"   Boosted (why): {self.sentences[idx][:40]}...")
            
            elif question_type == 'when':
                # Look for years (4-digit numbers)
                if re.search(r'\d{4}', sentence):
                    boosted_scores[i] += 0.2
                    print(f"   Boosted (when): {self.sentences[idx][:40]}...")
                # Look for time words
                if any(word in sentence for word in ['Ø³Ø§Ù„', 'Ø²Ù…Ø§Ù†', 'ØªØ§Ø±ÛŒØ®']):
                    boosted_scores[i] += 0.1
            
            elif question_type == 'where':
                # Look for location words
                if any(word in sentence for word in ['Ø´Ù‡Ø±', 'Ù…Ú©Ø§Ù†', 'ÙˆØ§Ù‚Ø¹', 'Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯']):
                    boosted_scores[i] += 0.2
                    print(f"   Boosted (where): {self.sentences[idx][:40]}...")
            
            elif question_type == 'who':
                # Look for person names (capitalized words)
                if re.search(r'[A-Z][a-z]+', sentence):
                    boosted_scores[i] += 0.15
                # Look for ØªÙˆØ³Ø·
                if 'ØªÙˆØ³Ø·' in sentence:
                    boosted_scores[i] += 0.2
                    print(f"   Boosted (who): {self.sentences[idx][:40]}...")
        
        # Return index with best boosted score
        best_idx_in_candidates = np.argmax(boosted_scores)
        return candidate_indices[best_idx_in_candidates]
    
    def get_stats(self):
        """Get statistics about the trained model"""
        return {
            'total_sentences': len(self.sentences),
            'vocabulary_size': len(self.vectorizer.get_feature_names_out()) if self.vectorizer else 0,
            'trained': self.vectorizer is not None,
            'stop_words_count': len(self.stop_words)
        }


def run_trained_qa_demo():
    """
    Demonstration of the trained QA system
    """
    print("=" * 80)
    print("ðŸŽ“ TRAINED PERSIAN QA SYSTEM DEMO")
    print("=" * 80)
    
    # Initialize system
    qa = TrainedPersianQA()
    
    # Training dataset
    training_data = [
        """
        Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û±Û³ ØªØ§Ø³ÛŒØ³ Ø´Ø¯. Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¯Ø± Ø´Ù‡Ø± ØªÙ‡Ø±Ø§Ù† ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡ Ø§Ø³Øª. 
        Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø§Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ù‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒØŒ Ù¾Ø²Ø´Ú©ÛŒ Ùˆ Ø¹Ù„ÙˆÙ… Ø§Ù†Ø³Ø§Ù†ÛŒ Ø§Ø³Øª. 
        Ø¯Ù„ÛŒÙ„ Ù…Ø¹Ø±ÙˆÙ Ø¨ÙˆØ¯Ù† Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¢Ù† Ø§Ø³Øª. 
        ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø­Ø¯ÙˆØ¯ ÛµÛ°,Û°Û°Û° Ù†ÙØ± Ø§Ø³Øª. 
        Ø±Ø´ØªÙ‡ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø¯Ø± Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªØ¯Ø±ÛŒØ³ Ù…ÛŒ Ø´ÙˆØ¯.
        """,
        """
        Ø´Ø±Ú©Øª Ú¯ÙˆÚ¯Ù„ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û¹Û¸ ØªÙˆØ³Ø· Ù„Ø±ÛŒ Ù¾ÛŒØ¬ Ùˆ Ø³Ø±Ú¯ÛŒ Ø¨Ø±ÛŒÙ† ØªØ§Ø³ÛŒØ³ Ø´Ø¯.
        Ø¯ÙØªØ± Ù…Ø±Ú©Ø²ÛŒ Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Mountain View Ú©Ø§Ù„ÛŒÙØ±Ù†ÛŒØ§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯.
        Ú¯ÙˆÚ¯Ù„ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø®ÙˆØ¯ Ù…Ø´Ù‡ÙˆØ± Ø§Ø³Øª.
        Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø³Ø§Ù„ Û²Û°Û°Û´ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø¹Ø±Ø¶Ù‡ Ø´Ø¯.
        Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§ØµÙ„ÛŒ Ú¯ÙˆÚ¯Ù„ Ø´Ø§Ù…Ù„ Ø§Ù†Ø¯Ø±ÙˆÛŒØ¯ØŒ ÛŒÙˆØªÛŒÙˆØ¨ Ùˆ Ø¬ÛŒÙ…ÛŒÙ„ Ù…ÛŒ Ø¨Ø§Ø´Ø¯.
        """,
        """
        Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒ Ø§Ø² ØªÚ©Ù†ÛŒÚ© Ù‡Ø§ Ùˆ Ø³ÛŒØ³ØªÙ… Ù‡Ø§ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±ÛŒ Ú¯ÙØªÙ‡ Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ Ù‡Ø¯Ù Ø¢Ù† Ù‡Ø§ ØªÙ‚Ù„ÛŒØ¯ Ùˆ Ø´Ø¨ÛŒÙ‡ Ø³Ø§Ø²ÛŒ Ø±ÙØªØ§Ø±Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø§Ù†Ø³Ø§Ù† Ù‡Ø§ Ø§Ø³Øª.
        Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ù‡Ø§ Ù…ÛŒ ØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ù†Ù†Ø¯ØŒ Ø§Ø² ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø®ÙˆØ¯ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù†Ø¯ Ùˆ Ø­ØªØ§ ØªØµÙ…ÛŒÙ…Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§ØªØ®Ø§Ø° Ú©Ù†Ù†Ø¯.
        """
    ]
    
    paragraph_names = ["Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù†", "Ú¯ÙˆÚ¯Ù„", "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÙŠ"]
    
    # Train the system
    qa.train(training_data, paragraph_names)
    
    # Test questions
    test_questions = [
        # About Tehran University
        "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø± Ú†Ù‡ Ø³Ø§Ù„ÛŒ ØªØ§Ø³ÛŒØ³ Ø´Ø¯ØŸ",
        "Ú†Ø±Ø§ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ù…Ø¹Ø±ÙˆÙ Ø§Ø³ØªØŸ",
        "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ú©Ø¬Ø§ ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ",
        "ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",
        
        # About Google
        "Ú¯ÙˆÚ¯Ù„ Ø¯Ø± Ú†Ù‡ Ø³Ø§Ù„ÛŒ ØªØ§Ø³ÛŒØ³ Ø´Ø¯ØŸ",
        "Ø¨Ù†ÛŒØ§Ù†Ú¯Ø°Ø§Ø±Ø§Ù† Ú¯ÙˆÚ¯Ù„ Ú†Ù‡ Ú©Ø³Ø§Ù†ÛŒ Ù‡Ø³ØªÙ†Ø¯ØŸ",
        "Ø¯ÙØªØ± Ù…Ø±Ú©Ø²ÛŒ Ú¯ÙˆÚ¯Ù„ Ú©Ø¬Ø§Ø³ØªØŸ",
        "Ú†Ø±Ø§ Ú¯ÙˆÚ¯Ù„ Ù…Ø´Ù‡ÙˆØ± Ø§Ø³ØªØŸ",
        "Ú¯ÙˆÚ¯Ù„ Ú†Ù‡ Ù…Ø­ØµÙˆÙ„Ø§ØªÛŒ Ø¯Ø§Ø±Ø¯ØŸ",
        
        # About AI
        "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú†ÛŒØ³ØªØŸ",
        "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ú†Ù‡ Ø³Ø§Ù„ÛŒ Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯ØŸ",
        "Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ú†ÛŒØ³ØªØŸ",
    ]
    
    # Get stats
    stats = qa.get_stats()
    print(f"\nðŸ“Š System Statistics:")
    print(f"   - Sentences in knowledge base: {stats['total_sentences']}")
    print(f"   - Vocabulary size: {stats['vocabulary_size']}")
    print(f"   - Stop words: {stats['stop_words_count']}")
    
    # Answer questions
    print("\n" + "=" * 80)
    print("â“ ANSWERING QUESTIONS")
    print("=" * 80)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*80}")
        print(f"Q{i}: {question}")
        
        result = qa.find_answer(question, top_k=4, threshold=0.05)
        
        print(f"\nâœ… Answer (confidence: {result['confidence']:.3f}):")
        print(f"   {result['answer']}")
        
        if result['source']:
            print(f"\n   Source: {result['source']['paragraph_name']}")


if __name__ == "__main__":
    run_trained_qa_demo()


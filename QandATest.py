# -*- coding: utf-8 -*-
"""
Improved QA System with Tie-Breaking Rules
"""

from hazm import Normalizer, sent_tokenize, word_tokenize, Stemmer

class ImprovedPersianQA:
    def __init__(self, min_score_threshold=0.3):
        """Initialize HAZM components"""
        self.normalizer = Normalizer()
        self.stemmer = Stemmer()
        self.processed_data = {}
        self.min_score_threshold = min_score_threshold
    
    def process_paragraph(self, paragraph):
        """
        Process and analyze a Persian paragraph
        """
        # Normalize the text
        normalized = self.normalizer.normalize(paragraph)
        
        # Tokenize into sentences
        sentences = sent_tokenize(normalized)
        
        # Process each sentence
        processed_sentences = []
        
        for sentence in sentences:
            # Tokenize words
            words = word_tokenize(sentence)
            
            # Get stems
            stems = [self.stemmer.stem(word) for word in words]
            
            processed_sentences.append({
                'original': sentence,
                'words': words,
                'stems': stems
            })
        
        # Store processed data
        self.processed_data = {
            'original': paragraph,
            'normalized': normalized,
            'sentences': processed_sentences
        }
        
        return self.processed_data
    
    def calculate_score(self, question_stems, sentence_stems, question):
        """
        Calculate a better matching score with semantic tie-breaking
        """
        # Remove common stop words from scoring
        stop_stems = {'Ø§Ø³', 'Ø§Ø³Øª', 'Ø´Ø¯', 'Ø´Ø¯Ù†', 'Ø¨Ø§Ø´Ø¯', 'Ù‡Ø§ÛŒ', 'Ù‡Ø§', 'ØŸ', '!', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²'}
        filtered_question_stems = [s for s in question_stems if s not in stop_stems]
        filtered_sentence_stems = [s for s in sentence_stems if s not in stop_stems]
        
        if not filtered_question_stems:
            return 0
        
        # Calculate intersection of important stems
        common_stems = set(filtered_question_stems) & set(filtered_sentence_stems)
        
        # Calculate base score as percentage of matching question stems
        base_score = len(common_stems) / len(filtered_question_stems)
        
        # === TIE-BREAKING RULES ===
        
        # Rule 1: Bonus for matching question type indicators
        question_lower = question.lower()
        bonus = 0
        
        # WHY questions - look for reason indicators
        if any(word in question_lower for word in ['Ú†Ø±Ø§', 'Ø¨Ù‡ Ú†Ù‡ Ø¯Ù„ÛŒÙ„', 'Ø¹Ù„Øª', 'Ø¯Ù„ÛŒÙ„']):
            reason_indicators = ['Ø¯Ù„ÛŒÙ„', 'Ø²ÛŒØ±Ø§', 'Ú†ÙˆÙ†', 'Ø¨Ù‡ Ø¯Ù„ÛŒÙ„', 'Ø¹Ù„Øª']
            if any(indicator in sentence_stems for indicator in reason_indicators):
                bonus += 0.3
                print("    + Bonus for reason indicator")
        
        # WHEN questions - look for time indicators
        elif any(word in question_lower for word in ['Ú©ÛŒ', 'Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ', 'Ú©Ø¯Ø§Ù… Ø³Ø§Ù„', 'Ú†Ù‡ Ø³Ø§Ù„ÛŒ']):
            time_indicators = ['Ø³Ø§Ù„', 'Ø²Ù…Ø§Ù†', 'ØªØ§Ø±ÛŒØ®']
            if any(indicator in sentence_stems for indicator in time_indicators):
                bonus += 0.3
                print("    + Bonus for time indicator")
        
        # WHERE questions - look for location indicators
        elif any(word in question_lower for word in ['Ú©Ø¬Ø§', 'Ú©Ø¯Ø§Ù… Ù…Ú©Ø§Ù†', 'Ú©Ø¯Ø§Ù… Ø´Ù‡Ø±', 'Ú©Ø¬Ø§Ø³Øª']):
            location_indicators = ['Ø´Ù‡Ø±', 'Ù…Ú©Ø§Ù†', 'Ù…ÙˆÙ‚Ø¹ÛŒØª', 'ÙˆØ§Ù‚Ø¹']
            if any(indicator in sentence_stems for indicator in location_indicators):
                bonus += 0.3
                print("    + Bonus for location indicator")
        
        # Rule 2: Bonus for matching the actual question word
        question_word_bonus = 0
        important_question_words = ['Ú†Ø±Ø§', 'Ú©ÛŒ', 'Ú©Ø¬Ø§', 'Ú†Ú¯ÙˆÙ†Ù‡', 'Ú†Ù‡', 'Ú©Ø¯Ø§Ù…']
        for q_word in important_question_words:
            if q_word in question_lower and q_word in sentence_stems:
                question_word_bonus += 0.2
                print(f"    + Bonus for question word '{q_word}'")
        
        # Rule 3: Penalty for sentences that are too generic
        generic_indicators = ['Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ø¯Ø§Ø±Ø§ÛŒ', 'Ù…ÛŒ Ø¨Ø§Ø´Ø¯']
        generic_count = sum(1 for word in sentence_stems if word in generic_indicators)
        penalty = generic_count * 0.05
        
        final_score = base_score + bonus + question_word_bonus - penalty
        
        return min(final_score, 1.0)  # Cap at 1.0
    
    def find_answer(self, question):
        """
        Find the answer to a question in the processed paragraph
        """
        if not self.processed_data:
            return "No paragraph processed yet!"
        
        # Normalize and process the question
        question_norm = self.normalizer.normalize(question)
        question_words = word_tokenize(question_norm)
        question_stems = [self.stemmer.stem(word) for word in question_words]
        
        print(f"\nğŸ” Question: {question}")
        print(f"Question stems (filtered): {[s for s in question_stems if s not in {'Ø§Ø³', 'ØŸ'}]}")
        
        # Find the best matching sentence
        best_sentence = None
        best_score = 0
        candidates = []  # Store all candidates for tie-breaking
        
        for sentence_data in self.processed_data['sentences']:
            sentence = sentence_data['original']
            sentence_stems = sentence_data['stems']
            
            # Calculate improved score with tie-breaking
            score = self.calculate_score(question_stems, sentence_stems, question)
            
            candidates.append({
                'sentence': sentence,
                'score': score,
                'stems': sentence_stems
            })
            
            print(f"  '{sentence[:40]}...' | Score: {score:.2f}")
            
            if score > best_score:
                best_score = score
                best_sentence = sentence
        
        # Final tie-breaking: if multiple have same score, use additional rules
        if best_score >= self.min_score_threshold:
            top_candidates = [c for c in candidates if c['score'] == best_score]
            
            if len(top_candidates) > 1:
                print(f"  Tie detected between {len(top_candidates)} candidates with score {best_score:.2f}")
                best_sentence = self._break_tie(question, top_candidates)
            
            print(f"âœ“ Best match selected with score: {best_score:.2f}")
            return best_sentence
        else:
            print(f"âœ— No good match found (best score: {best_score:.2f}, threshold: {self.min_score_threshold})")
            return "Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
    
    def _break_tie(self, question, candidates):
        """
        Break ties between multiple candidates with the same score
        """
        question_lower = question.lower()
        
        # Rule 1: Prefer sentences that contain the actual question word
        for candidate in candidates:
            sentence_stems = candidate['stems']
            
            # For WHY questions, prefer sentences with reason words
            if any(word in question_lower for word in ['Ú†Ø±Ø§', 'Ø¯Ù„ÛŒÙ„', 'Ø¹Ù„Øª']):
                reason_words = ['Ø¯Ù„ÛŒÙ„', 'Ø²ÛŒØ±Ø§', 'Ú†ÙˆÙ†', 'Ø¹Ù„Øª']
                if any(word in sentence_stems for word in reason_words):
                    print(f"  Tie broken: selected sentence with reason word")
                    return candidate['sentence']
            
            # For WHEN questions, prefer sentences with numbers (years)
            elif any(word in question_lower for word in ['Ú©ÛŒ', 'Ø²Ù…Ø§Ù†', 'Ø³Ø§Ù„']):
                # Look for 4-digit numbers (years)
                import re
                if re.search(r'\d{4}', candidate['sentence']):
                    print(f"  Tie broken: selected sentence with year")
                    return candidate['sentence']
            
            # For WHERE questions, prefer sentences with location words
            elif any(word in question_lower for word in ['Ú©Ø¬Ø§', 'Ù…Ú©Ø§Ù†', 'Ø´Ù‡Ø±']):
                location_words = ['Ø´Ù‡Ø±', 'Ù…Ú©Ø§Ù†', 'ÙˆØ§Ù‚Ø¹', 'Ù…ÙˆÙ‚Ø¹ÛŒØª']
                if any(word in sentence_stems for word in location_words):
                    print(f"  Tie broken: selected sentence with location word")
                    return candidate['sentence']
        
        # Rule 2: If still tied, prefer longer sentences (more informative)
        longest_candidate = max(candidates, key=lambda x: len(x['sentence']))
        print(f"  Tie broken: selected longest sentence")
        return longest_candidate['sentence']


def run_improved_qa_test():
    """
    Run QA test with improved scoring and tie-breaking
    """
    print("=" * 70)
    print("ğŸ§  IMPROVED QA SYSTEM WITH TIE-BREAKING")
    print("=" * 70)
    
    # Sample Persian paragraph
    paragraph = """
    Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û±Û³ ØªØ§Ø³ÛŒØ³ Ø´Ø¯. Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¯Ø± Ø´Ù‡Ø± ØªÙ‡Ø±Ø§Ù† ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡ Ø§Ø³Øª. 
    Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø§Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ù‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒØŒ Ù¾Ø²Ø´Ú©ÛŒ Ùˆ Ø¹Ù„ÙˆÙ… Ø§Ù†Ø³Ø§Ù†ÛŒ Ø§Ø³Øª. 
    Ø¯Ù„ÛŒÙ„ Ù…Ø¹Ø±ÙˆÙ Ø¨ÙˆØ¯Ù† Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¢Ù† Ø§Ø³Øª. 
    ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø­Ø¯ÙˆØ¯ ÛµÛ°,Û°Û°Û° Ù†ÙØ± Ø§Ø³Øª. 
    Ø±Ø´ØªÙ‡ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø¯Ø± Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªØ¯Ø±ÛŒØ³ Ù…ÛŒ Ø´ÙˆØ¯.
    """
    
    # Initialize improved QA system
    qa_system = ImprovedPersianQA(min_score_threshold=0.3)
    
    # Process the paragraph
    print("\nğŸ“– Processing paragraph...")
    qa_system.process_paragraph(paragraph)
    
    # Test questions
    test_questions = [
        "Ú†Ø±Ø§ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ù…Ø¹Ø±ÙˆÙ Ø§Ø³ØªØŸ",  # Should match the "Ø¯Ù„ÛŒÙ„ Ù…Ø¹Ø±ÙˆÙ Ø¨ÙˆØ¯Ù†" sentence
        "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ø¯Ø± Ú†Ù‡ Ø³Ø§Ù„ÛŒ ØªØ§Ø³ÛŒØ³ Ø´Ø¯ØŸ",  # Should match the first sentence
        "ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ",  # Should match the numbers sentence
        "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ú©Ø¬Ø§ ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ",  # Should match location sentence
    ]
    
    print("\n" + "=" * 70)
    print("â“ IMPROVED QUESTION ANSWERING 1")
    print("=" * 70)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. {question}")
        answer = qa_system.find_answer(question)
        print(f"   ğŸ¤– Ù¾Ø§Ø³Ø®: {answer}")



    paragraph2 = """
    Ø´Ø±Ú©Øª Ú¯ÙˆÚ¯Ù„ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û¹Û¸ ØªÙˆØ³Ø· Ù„Ø±ÛŒ Ù¾ÛŒØ¬ Ùˆ Ø³Ø±Ú¯ÛŒ Ø¨Ø±ÛŒÙ† ØªØ§Ø³ÛŒØ³ Ø´Ø¯..
    Ø¯ÙØªØ± Ù…Ø±Ú©Ø²ÛŒ Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± mountain view Ú©Ø§Ù„ÛŒÙØ±Ù†ÛŒØ§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯..
    Ú¯ÙˆÚ¯Ù„ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø®ÙˆØ¯ Ù…Ø´Ù‡ÙˆØ± Ø§Ø³Øª..
    Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø³Ø§Ù„ Û²Û°Û°Û´ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø¹Ø±Ø¶Ù‡ Ø´Ø¯..
    Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§ØµÙ„ÛŒ Ú¯ÙˆÚ¯Ù„ Ø´Ø§Ù…Ù„ Ø§Ù†Ø¯Ø±ÙˆÛŒØ¯ØŒ ÛŒÙˆØªÛŒÙˆØ¨ Ùˆ Ø¬ÛŒÙ…ÛŒÙ„ Ù…ÛŒ Ø¨Ø§Ø´Ø¯.
    """
    # Initialize improved QA system
    qa_system = ImprovedPersianQA(min_score_threshold=0.3)

    # Process the paragraph
    print("\nğŸ“– Processing paragraph...")
    qa_system.process_paragraph(paragraph2)


    questions = [
        "Ú¯ÙˆÚ¯Ù„ Ø¯Ø± Ú†Ù‡ Ø³Ø§Ù„ÛŒ ØªØ§Ø³ÛŒØ³ Ø´Ø¯ØŸ",
        "Ø¨Ù†ÛŒØ§Ù†Ú¯Ø°Ø§Ø±Ø§Ù† Ú¯ÙˆÚ¯Ù„ Ú†Ù‡ Ú©Ø³Ø§Ù†ÛŒ Ù‡Ø³ØªÙ†Ø¯ØŸ",
        "Ø¯ÙØªØ± Ù…Ø±Ú©Ø²ÛŒ Ú¯ÙˆÚ¯Ù„ Ú©Ø¬Ø§Ø³ØªØŸ",
        "Ú†Ø±Ø§ Ú¯ÙˆÚ¯Ù„ Ù…Ø´Ù‡ÙˆØ± Ø§Ø³ØªØŸ",
        "Ú¯ÙˆÚ¯Ù„ Ú†Ù‡ Ù…Ø­ØµÙˆÙ„Ø§ØªÛŒ Ø¯Ø§Ø±Ø¯ØŸ"
    ]

    print("\n" + "=" * 70)
    print("â“ IMPROVED QUESTION ANSWERING 2")
    print("=" * 70)

    for i, question in enumerate(questions, 1):
        print(f"\n{i}. {question}")
        answer = qa_system.find_answer(question)
        print(f"   ğŸ¤– Ù¾Ø§Ø³Ø®: {answer}")



if __name__ == "__main__":
    run_improved_qa_test()